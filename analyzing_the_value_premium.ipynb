{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93f40c6a",
   "metadata": {},
   "source": [
    "# Analyzing the Value Premium\n",
    "February 2022\n",
    "\n",
    "*July 2022 Note: Since the start of this notebook in February 2022, the valuation spread between cheap and expensive stocks reverted further to the mean as additional inflationary pressure shifted the government yield curve upwards. Expensive stocks lost ~10% and cheap stocks were fairly flat. The value premium in small stocks has appreciated more than in large stocks.*\n",
    "<br>\n",
    "*I expect the large cap valuation spread to revert further to the mean in the coming months.*\n",
    "\n",
    "## Introduction\n",
    "In 1992, Eugene Fama and Kenneth French introduced a Three-Factor Model in their famous paper \"The Cross-Section of Expected Stock Returns\" that extends William Sharpe's CAPM with a Size Factor (SMB) and a Value Factor (HML):\n",
    "<br><br>\n",
    "$$ E(r) = \\alpha + \\beta_{1} *  (MKT-R_f) + \\beta_{2} * SMB + \\beta_{3} * HML $$\n",
    "<br>\n",
    "Given the parsimonous structure, the theoretical existence of state variables that investors want to hedge against and the poor empirical performance of the CAPM, it has become, together with Fama/French's (FF) extended 5-Factor Model from 2015, the standard model to price the cross-section of equity returns.\n",
    "<br>\n",
    "However, since the publication of the paper, both factors failed to deliver any premium out-of-sample and the value premium (HML) in particular currently experiences a 20-year drawdown, erasing half of its entire 100-year cumulative outperformance. Sceptics claim that both premia are unrelated to risk and that the model is a result of data snooping. Once the existence of a new factor becomes common knowledge and investors adapt and try to exploit it, it should, via arbitrage, ultimately vanish. \n",
    "<br>\n",
    "While asset pricing theory points to factors aside from the sensitivity to the market portfolio and although the four additional Fama/French factors are theoretically sound as they directly relate to the pricing equation, which in turn might point to a set of state variables in Robert Merton's ICAPM, it is unclear whether they can indeed proxy the underlying risk factors. This notebook puts the theory aside and provides a quantitative analysis of the drivers of the value premium and shows that the value premium still exists and is possibly a historically attractive diversifier in the current market environment.\n",
    "\n",
    "**Table of Contents:**\n",
    "1. [Descriptive&nbsp;Analysis](#1.&nbsp;Descriptive&nbsp;Analysis)\n",
    "2. [Stochastic&nbsp;Analysis](#2.&nbsp;Stochastic&nbsp;Analysis)\n",
    "3. [Relative&nbsp;Valuations](#3.&nbsp;Relative&nbsp;Valuations)\n",
    "4. [Explaining&nbsp;Variables](#4.&nbsp;Explaining&nbsp;Variables)\n",
    "5. [Time-Series&nbsp;Predictability](#5.&nbsp;Time-Series&nbsp;Predictability)\n",
    "6. [ETF/Stock&nbsp;Selection](#6.&nbsp;ETF/Stock&nbsp;Selection)\n",
    "7. [Summary](#7.&nbsp;Summary)\n",
    "\n",
    "**Data sources:**\n",
    "* [Kenneth French's Portfolio Database](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html)\n",
    "* [Robert Shiller's History of CAPE](http://www.econ.yale.edu/~shiller/data.htm)\n",
    "* [Bond Rates from the Federal Feserve Bank of St. Louis](https://fred.stlouisfed.org/)\n",
    "\n",
    "**Dependencies:**\n",
    "* numpy\n",
    "* pandas\n",
    "* matplotlib\n",
    "* statsmodels\n",
    "* scipy\n",
    "* [finance_data](https://github.com/Jon-Nie/finance_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530b5c2e",
   "metadata": {},
   "source": [
    "#### Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96163055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.api import OLS, add_constant\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.stats import (\n",
    "    jarque_bera,\n",
    "    norm,\n",
    "    gaussian_kde\n",
    ")\n",
    "from finance_data import (\n",
    "    FREDReader,\n",
    "    FrenchReader,\n",
    "    MSCIReader,\n",
    "    YahooReader,\n",
    "    shiller_cape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856e773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ace6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "shiller_data = shiller_cape()[\"CAPE\"]\n",
    "shiller_rate = shiller_cape()[\"10-Year Interest Rate\"]\n",
    "\n",
    "rate_1yr = FREDReader(\"DGS1\").historical_data().resample(\"MS\").first() / 100\n",
    "rate_1yr.columns = [\"1-Year Interest Rate\"]\n",
    "rate_10yr = FREDReader(\"DGS10\").historical_data().resample(\"MS\").first() / 100\n",
    "rate_10yr.columns = [\"10-Year Interest Rate\"]\n",
    "\n",
    "ff3 = FrenchReader(\"F-F_Research_Data_Factors\").read()[\"Main\"] / 100\n",
    "ff5 = FrenchReader(\"F-F_Research_Data_5_Factors_2x3\").read()[\"Main\"] / 100\n",
    "mom = FrenchReader(\"F-F_Momentum_Factor\").read()[\"Main\"] / 100\n",
    "\n",
    "df = pd.concat([ff3, mom, shiller_data], axis=1).dropna()\n",
    "df[\"RMW\"] = ff5[\"RMW\"]\n",
    "df[\"CMA\"] = ff5[\"CMA\"]\n",
    "df = df[[\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"Mom\", \"RF\", \"CAPE\"]]\n",
    "\n",
    "bm_sorted_data = FrenchReader(\"Portfolios_Formed_on_BE-ME\").read()\n",
    "bm_size_sorted_data = FrenchReader(\"6_Portfolios_2x3\").read()\n",
    "\n",
    "ep_sorted_data = FrenchReader(\"Portfolios_Formed_on_E-P\").read()\n",
    "ep_size_sorted_data = FrenchReader(\"6_Portfolios_ME_EP_2x3\").read()\n",
    "\n",
    "cfp_sorted_data = FrenchReader(\"Portfolios_Formed_on_CF-P\").read()\n",
    "cfp_size_sorted_data = FrenchReader(\"6_Portfolios_ME_CFP_2x3\").read()\n",
    "\n",
    "bm_size_5_5_sorted_data = FrenchReader(\"25_Portfolios_5x5\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86315f98",
   "metadata": {},
   "source": [
    "## 1.&nbsp;Descriptive&nbsp;Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff339f8",
   "metadata": {},
   "source": [
    "Starting with descriptive statistics, no factor of Fama/French's five-factor model, except for the equity premium itself, has delivered a significant premium since the publication of the respective model in 1992 and 2015, which questions the models out-of-sample performance to price equity risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e6b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Since 1992,...\")\n",
    "for factor in (\"Mkt-RF\", \"SMB\", \"HML\"):   \n",
    "    ols_fit = OLS(df.loc[\"1992-01-01\":, factor], [1 for _ in range(len(df.loc[\"1992-01-01\":, :].index))]).fit()\n",
    "    mean = ols_fit.params[\"const\"]\n",
    "    tstat = ols_fit.tvalues[\"const\"]\n",
    "    print(f\"...the {factor} factor returned an average of {mean:.2%} per month with a t-statistic of {tstat:.2f}.\")\n",
    "print(\"\")\n",
    "print(\"Since 2015,...\")\n",
    "for factor in (\"RMW\", \"CMA\"):   \n",
    "    ols_fit = OLS(df.loc[\"2015-01-01\":, factor], [1 for _ in range(len(df.loc[\"2015-01-01\":, :].index))]).fit()\n",
    "    mean = ols_fit.params[\"const\"]\n",
    "    tstat = ols_fit.tvalues[\"const\"]\n",
    "    print(f\"...the {factor} factor returned an average of {mean:.2%} per month with a t-statistic of {tstat:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a332c",
   "metadata": {},
   "source": [
    "While all four factors failed to deliver any premium out-of-sample, I will only discuss the value premium as it is the factor that has recently shown the worst performance and because it has a long tradition in equity markets, ranging back to Benjamin Graham in the early 20th century."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ef5af",
   "metadata": {},
   "source": [
    "Since 2007, the value premium had its largest drawdown in history and shed about half of its entire cumulative outperformance since 1927 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49b0a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[\"HML Cumulative\"] = (1+df[\"HML\"]).cumprod()\n",
    "df[\"HML Drawdown\"] = df[\"HML Cumulative\"] / df[\"HML Cumulative\"].cummax() - 1\n",
    "\n",
    "figure, axes = plt.subplots(2, gridspec_kw={'height_ratios': [5, 3]}, figsize=(15,8))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "figure.suptitle(\"A Century of the Value Premium\", fontsize=18, fontweight=\"bold\")\n",
    "\n",
    "axes[0].plot(np.log(df[\"HML Cumulative\"]), color=\"#4459c2\")\n",
    "axes[0].axvline(x=(1992-1970)*365, label=\"Paper Publish Year\", color=\"#d66b6b\")\n",
    "\n",
    "axes[0].set_title(\"Cumulative HML Performance\", size=16)\n",
    "axes[0].set_xlabel(\"Year\", size=14)\n",
    "axes[0].set_ylabel(\"Cumulative Return (Logarithmized)\", size=14)\n",
    "axes[0].tick_params(axis=\"x\", labelsize=13)\n",
    "\n",
    "axes[0].legend(fontsize=11, bbox_to_anchor=(0.8,0.2));\n",
    "\n",
    "axes[1].plot(df[\"HML Drawdown\"], color=\"#4459c2\")\n",
    "axes[1].axvline(x=(1992-1970)*365, label=\"Paper Publish Year\", color=\"#d66b6b\")\n",
    "\n",
    "axes[1].set_title(\"HML Drawdown\", size=16)\n",
    "axes[1].set_xlabel(\"Year\", size=14)\n",
    "axes[1].set_ylabel(\"Drawdown\", size=14)\n",
    "axes[1].tick_params(axis=\"x\", labelsize=13)\n",
    "\n",
    "yticks = [-0.6,-0.4,-0.2, 0]\n",
    "axes[1].set_yticks(yticks)\n",
    "axes[1].set_yticklabels([f\"{tick:.0%}\" for tick in yticks], size=13)\n",
    "\n",
    "axes[1].legend(fontsize=11, loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb522bc",
   "metadata": {},
   "source": [
    "... and the failure of the value premium was both present in large and small caps, although higher in magnitude in the large cap segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae313a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[\"Large High B/M\"] = bm_size_sorted_data[\"Average Value Weighted Returns Monthly\"][\"BIG HiBM\"] / 100\n",
    "df[\"Large Low B/M\"] = bm_size_sorted_data[\"Average Value Weighted Returns Monthly\"][\"BIG LoBM\"] / 100\n",
    "df[\"Small High B/M\"] = bm_size_sorted_data[\"Average Value Weighted Returns Monthly\"][\"SMALL HiBM\"] / 100\n",
    "df[\"Small Low B/M\"] = bm_size_sorted_data[\"Average Value Weighted Returns Monthly\"][\"SMALL LoBM\"] / 100\n",
    "\n",
    "df[\"Large Value Premium\"] = df[\"Large High B/M\"] - df[\"Large Low B/M\"] \n",
    "df[\"Large Value Premium Cumulative\"] = (1+df[\"Large Value Premium\"]).cumprod()\n",
    "df[\"Small Value Premium\"] = df[\"Small High B/M\"] - df[\"Small Low B/M\"]\n",
    "df[\"Small Value Premium Cumulative\"] = (1+df[\"Small Value Premium\"]).cumprod()\n",
    "\n",
    "df[\"Large Value Premium Drawdown\"] = df[\"Large Value Premium Cumulative\"] / df[\"Large Value Premium Cumulative\"].cummax() - 1\n",
    "df[\"Small Value Premium Drawdown\"] = df[\"Small Value Premium Cumulative\"] / df[\"Small Value Premium Cumulative\"].cummax() - 1\n",
    "\n",
    "figure, axes = plt.subplots(2, gridspec_kw={'height_ratios': [5, 3]}, figsize=(15,8))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "figure.suptitle(\"Value Premium of Small- and Large-Caps\", fontsize=18, fontweight=\"bold\")\n",
    "\n",
    "axes[0].plot(np.log((1+df[\"Large Value Premium\"]).cumprod()), label=\"Large Caps\")\n",
    "axes[0].plot(np.log((1+df[\"Small Value Premium\"]).cumprod()), label=\"Small Caps\", color=\"red\")\n",
    "\n",
    "axes[0].set_title(\"Cumulative Value Premium\", size=16)\n",
    "axes[0].set_xlabel(\"Year\", fontsize=14)\n",
    "axes[0].set_ylabel(\"Cumulative Premium (Logarithmized)\", fontsize=14)\n",
    "\n",
    "yticks1 = axes[0].get_yticks().tolist()\n",
    "axes[0].set_yticks(yticks1)\n",
    "axes[0].legend(fontsize=12, bbox_to_anchor=(0.2, 1))\n",
    "\n",
    "axes[1].plot(df[\"Large Value Premium Drawdown\"], color=\"#4459c2\", linewidth=0.95, label=\"Large Caps\")\n",
    "axes[1].plot(df[\"Small Value Premium Drawdown\"], color=\"red\", linewidth=0.95, label=\"Small Caps\")\n",
    "\n",
    "axes[1].set_title(\"Drawdown\", size=16)\n",
    "axes[1].set_xlabel(\"Year\", size=14)\n",
    "axes[1].set_ylabel(\"Drawdown\", size=14)\n",
    "axes[1].tick_params(axis=\"x\", labelsize=13)\n",
    "\n",
    "yticks = [-0.6,-0.4,-0.2, 0]\n",
    "axes[1].set_yticks(yticks)\n",
    "axes[1].set_yticklabels([f\"{tick:.0%}\" for tick in yticks], size=13)\n",
    "axes[1].legend(fontsize=12, bbox_to_anchor=(0, 0, 0.9, 0.98));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e362819",
   "metadata": {},
   "source": [
    "While some people argue that book values are a flawed measure of valuations as highly profitable technology companies with low capital requirements and hence low book-values become more relevant today, the drawdowns look similar if we take net-earnings or cashflows instead of book-values.\n",
    "<br>\n",
    "However, the recent drawdowns of value premia based on E/P and CF/P begin a few years later and are not as large in magnitude as the recent drawdown of the book-value-based value premium.\n",
    "<br>\n",
    "*(Note that the data for other variables is constrained to a later starting date)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff069c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Large Value Premium E/P\"] = (ep_size_sorted_data[\"Value Weight Average Returns Monthly\"][\"BIG HiEP\"] - ep_size_sorted_data[\"Value Weight Average Returns Monthly\"][\"BIG LoEP\"]) / 100\n",
    "df[\"Large Value Premium E/P Cumulative\"] = (1+df[\"Large Value Premium E/P\"]).cumprod()\n",
    "df[\"Small Value Premium E/P\"] = (ep_size_sorted_data[\"Value Weight Average Returns Monthly\"][\"SMALL HiEP\"] - ep_size_sorted_data[\"Value Weight Average Returns Monthly\"][\"SMALL LoEP\"]) / 100\n",
    "df[\"Small Value Premium E/P Cumulative\"] = (1+df[\"Small Value Premium E/P\"]).cumprod()\n",
    "\n",
    "df[\"HML E/P\"] = (df[\"Large Value Premium E/P\"] + df[\"Small Value Premium E/P\"]) / 2\n",
    "df[\"HML E/P Cumulative\"] = (1+df[\"HML E/P\"]).cumprod()\n",
    "df[\"HML E/P Drawdown\"] = df[\"HML E/P Cumulative\"] / df[\"HML E/P Cumulative\"].cummax() - 1\n",
    "\n",
    "df[\"Large Value Premium CF/P\"] = (cfp_size_sorted_data[\"Value Weight Average Returns Monthly\"][\"BIG HiCFP\"] - cfp_size_sorted_data[\"Value Weight Average Returns Monthly\"][\"BIG LoCFP\"]) / 100\n",
    "df[\"Large Value Premium CF/P Cumulative\"] = (1+df[\"Large Value Premium CF/P\"]).cumprod()\n",
    "df[\"Small Value Premium CF/P\"] = (cfp_size_sorted_data[\"Value Weight Average Returns Monthly\"][\"SMALL HiCFP\"] - cfp_size_sorted_data[\"Value Weight Average Returns Monthly\"][\"SMALL LoCFP\"]) / 100\n",
    "df[\"Small Value Premium CF/P Cumulative\"] = (1+df[\"Small Value Premium CF/P\"]).cumprod()\n",
    "\n",
    "df[\"HML CF/P\"] = (df[\"Large Value Premium CF/P\"] + df[\"Small Value Premium CF/P\"]) / 2\n",
    "df[\"HML CF/P Cumulative\"] = (1+df[\"HML CF/P\"]).cumprod()\n",
    "df[\"HML CF/P Drawdown\"] = df[\"HML CF/P Cumulative\"] / df[\"HML CF/P Cumulative\"].cummax() - 1\n",
    "\n",
    "figure, axes = plt.subplots(2, figsize=(15,8), gridspec_kw={'height_ratios': [5,3]})\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "figure.suptitle(\"Value Premia based on Different Variables\", fontsize=18, fontweight=\"bold\")\n",
    "\n",
    "axes[0].plot(np.log(df[\"HML E/P Cumulative\"]), color=\"#4459c2\", label=\"E/P\")\n",
    "axes[0].plot(np.log(df[\"HML CF/P Cumulative\"]), color=\"red\", label=\"CF/P\")\n",
    "axes[0].set_title(\"Cumulative HML based on E/P and CF/P\", size=16)\n",
    "axes[0].set_xlabel(\"Year\", size=14)\n",
    "axes[0].set_ylabel(\"Cumulative Return\", size=14)\n",
    "axes[0].legend(fontsize=12)\n",
    "\n",
    "axes[1].plot(df[\"HML E/P Drawdown\"], color=\"#4459c2\", linewidth=0.95, label=\"E/P\")\n",
    "axes[1].plot(df[\"HML CF/P Drawdown\"], color=\"red\", linewidth=0.95, label=\"CF/P\")\n",
    "\n",
    "axes[1].set_title(\"Drawdowns\", size=16)\n",
    "axes[1].set_xlabel(\"Year\", size=14)\n",
    "axes[1].set_ylabel(\"Drawdown\", size=14)\n",
    "axes[1].tick_params(axis=\"x\", labelsize=13)\n",
    "\n",
    "yticks = [-0.6,-0.4,-0.2, 0]\n",
    "axes[1].set_yticks(yticks)\n",
    "axes[1].set_yticklabels([f\"{tick:.0%}\" for tick in yticks], size=13)\n",
    "axes[1].legend(fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f19246",
   "metadata": {},
   "source": [
    "The correlations between alternative measures of the value premium also highlight that it does not matter which fundamental variable we use to measure the value premium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea50f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(df[[\"HML\", \"HML E/P\", \"HML CF/P\"]].corr(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcae6c1a",
   "metadata": {},
   "source": [
    "To sum it up, the value factor has been terrible recently, no matter of the size of the companies and the variable used. However, there are at least two arguments, considered individually or together, that might invalidate the claim that the recent data points to a vanished value premium:\n",
    "1. The underperformance is the result of chance.\n",
    "2. Macroeconomic/fundamentals shifts affected cheap and expensive stocks differently, resulting either in changing valuations or in different unexpected underlying fundamental performances of the companies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3871c86",
   "metadata": {},
   "source": [
    "## 2.&nbsp;Stochastic&nbsp;Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25adc6d9",
   "metadata": {},
   "source": [
    "Given that all factors carry a sizable amount of risk in the form of volatility, it should be no surprise that a factor, described by a stochastic process with positive mean can underperform for a lengthy period. For example, for an iid normally distributed white-noise process with a yearly mean return of 10% and 20% yearly volatility, there is still a probability of 2% that the process realizes a mean return lower than 0% over a period of 15 years. Likewise, the US market portfolio showed multiple periods of 15-year underperformance against bonds.\n",
    "<br>\n",
    "To see whether the underperformance is due to chance, one can compute the pvalues of the realized premium, given the prior return distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d5ea43",
   "metadata": {},
   "source": [
    "Pior to publication in 1992, the Value Premium had delivered an average monthly excess return of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f48852",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mean = df.loc[:\"1992-01-01\", \"HML\"].mean()\n",
    "print(f\"{prior_mean:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac55fcd",
   "metadata": {},
   "source": [
    "with a monthly volatility of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_std = df.loc[:\"1992-01-01\", \"HML\"].std()\n",
    "print(f\"{prior_std:.2%}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84ea136",
   "metadata": {},
   "source": [
    "Assuming a gaussian distribution and stationarity between pre- and post-publication (i.e. the same mean and volatility pre- and post-publication), we can compute the probability that the value premium returns an average return below the realized average post-publication return. Given the increasing number of factor ETFs to give retail investors access to alternative factor premia and investors needing some time to fully adapt to new academic research, I do the same for the subsamples of 1997, 2002, 2007 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef29cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The respective sample size of each considered subsample is as follows:\\n\")\n",
    "for year in (1992, 1997, 2002, 2007):\n",
    "    sample_size = df.loc[f\"{year}-01-01\":, \"HML\"].count()\n",
    "    print(f\"The sample size spanning from January {year} to today is {sample_size} months.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4939803",
   "metadata": {},
   "source": [
    "Turning to the probabilities, we get the following values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in (1992, 1997, 2002, 2007):\n",
    "    sample_size = df.loc[f\"{year}-01-01\":, \"HML\"].count()\n",
    "    std_err = df.loc[f\"{year}-01-01\":, \"HML\"].std() / np.sqrt(sample_size)\n",
    "    realized_mean = df.loc[f\"{year}-01-01\":, \"HML\"].mean()\n",
    "    mean_diff = realized_mean - prior_mean\n",
    "    z_value = mean_diff / std_err\n",
    "    p_value = norm.cdf(z_value)\n",
    "    print(f\"For the subsample from January {year} to today, the probability of an average return less than {realized_mean:.2%} per month is {p_value:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45de4b5",
   "metadata": {},
   "source": [
    "So far, one could certainly (and possibly unsurpringly) conclude at some reasonable confidence level that the value premium has a structural break (i.e. a linear mean-shift over time as investors adapt to reserach) between pre- and post-publication of the paper.\n",
    "However, running the same for a hypothesis of a zero mean-return post-publication gives the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db4da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in (1992, 1997, 2002, 2007):\n",
    "    sample_size = df.loc[f\"{year}-01-01\":, \"HML\"].count()\n",
    "    std_err = df.loc[f\"{year}-01-01\":, \"HML\"].std() / np.sqrt(sample_size)\n",
    "    realized_mean = df.loc[f\"{year}-01-01\":, \"HML\"].mean()\n",
    "    mean_diff = realized_mean - 0\n",
    "    z_value = mean_diff / std_err\n",
    "    p_value = norm.cdf(z_value)\n",
    "    print(f\"For the subsample from January {year} to today, the probability of an average return less than {realized_mean:.2%} per month is {p_value:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b929b8d6",
   "metadata": {},
   "source": [
    "Therefore, based on the assumptions and the data, it is not possible to argue that the value premium is dead.\n",
    "<br>\n",
    "One issue though is that the empirical distribution of the value premium, like many financial time-series, is non-normal, although I assumed normality until now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d515739",
   "metadata": {},
   "source": [
    "Its skewness is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_skew = df.loc[:\"1992-01-01\", \"HML\"].skew()\n",
    "np.round(prior_skew, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eecf73",
   "metadata": {},
   "source": [
    "and its excess kurtosis is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_kurt = df.loc[:\"1992-01-01\", \"HML\"].kurt()\n",
    "print(np.round(prior_kurt, 2), \",\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525becd",
   "metadata": {},
   "source": [
    "both well above that of a normal distribution. Accordingly, a jarque-bera test easily rejects the hypothesis of a normal distribution at the 1% level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "jb = jarque_bera(df.loc[:\"1992-01-01\", \"HML\"]).pvalue\n",
    "print(f\"{jb:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0cb82",
   "metadata": {},
   "source": [
    "The empirical distribution compared to a normal distribution with same mean and variance also highlights the empirical leptokurtic structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e937129",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_density = gaussian_kde(df.loc[:\"1992-01-01\", \"HML\"])\n",
    "\n",
    "x = np.linspace(-0.2, 0.2, 1000)\n",
    "plt.plot(x, estimated_density(x), label=\"Estimated Empirical Density\")\n",
    "plt.plot(x, norm.pdf(x, prior_mean, prior_std), label=\"Gaussian Density with same Mean and Variance\", color=\"red\")\n",
    "\n",
    "plt.title(\"Density Comparison\", fontsize=18, fontweight=\"bold\")\n",
    "plt.ylabel(\"Density\", fontsize=14)\n",
    "plt.xlabel(\"Monthly Return\", fontsize=14)\n",
    "\n",
    "xticks = plt.xticks()[0]\n",
    "plt.xticks(xticks, [f\"{ret:.0%}\" for ret in xticks], fontsize=13)\n",
    "yticks = plt.yticks()[0]\n",
    "plt.yticks(yticks[1:], yticks[1:], fontsize=13)\n",
    "\n",
    "plt.legend(fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb7d48",
   "metadata": {},
   "source": [
    "Therefore, it is important to account for non-normality and compute the same probabilities before by incorporating the empirical distribution instead of assuming normality. For that, I bootstrap intervals by drawing n times (n=sample size) with replacement from the empirical distribution until January 1992. I do it 100.000 times for each subsample and compute the probability of an average return less than the respective realized return by counting the ocurrences and diving it by the total number of bootstrap samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2042dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = {}\n",
    "sample_size = df.loc[\"1992-01-01\":, \"HML\"].count()\n",
    "for i in range(100_000):\n",
    "    series[i] = pd.Series(np.random.choice(df.loc[:f\"1992-01-01\", \"HML\"], sample_size))\n",
    "bootstrapped_df = pd.DataFrame(series)\n",
    "bootstrapped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa35bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in (1992, 1997, 2002, 2007):\n",
    "    realized_mean = df.loc[f\"{year}-01-01\":, \"HML\"].mean()\n",
    "    sample_size = df.loc[f\"{year}-01-01\":, \"HML\"].count()\n",
    "    prob = (bootstrapped_df.iloc[:sample_size, :].mean().sort_values().reset_index(drop=True) < realized_mean).sum() / 100_000\n",
    "    print(f\"For the subsample from January {year} to today, the probability of an average return less than {realized_mean:.2%} per month is {prob:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a52c28f",
   "metadata": {},
   "source": [
    "And the same results in graphical representation look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b4fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 2, figsize=(16,12))\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "figure.suptitle(\"Bootstrapped Distributions and realized Premia\", fontsize=18, fontweight=\"bold\", y=0.95)\n",
    "\n",
    "for year, (x, y) in zip((1992, 1997, 2002, 2007), ((0,0), (0,1), (1,0), (1,1))):\n",
    "    sample_size = df.loc[f\"{year}-01-01\":, \"HML\"].count()\n",
    "    axes[x,y].hist(bootstrapped_df.iloc[:sample_size, :].mean(), bins=70, label=\"Bootstrapped Distribution\")\n",
    "    axes[x,y].axvline(df.loc[f\"{year}-01-01\":, \"HML\"].mean(), color=\"red\", label=\"Realized Average Premium\")\n",
    "    \n",
    "    axes[x,y].set_title(f\"Subsample starting {year}\", fontsize=14)\n",
    "    axes[x,y].set_xlabel(\"Monthly Return\", fontsize=14)\n",
    "    axes[x,y].set_ylabel(\"Observations\", fontsize=14)\n",
    "    \n",
    "    xticks = np.linspace(-0.015, 0.015, 7)\n",
    "    axes[x,y].set_xticks(xticks)\n",
    "    axes[x,y].set_xticklabels([f\"{ret:.2%}\" for ret in xticks], fontsize=12)\n",
    "    yticks = axes[x,y].get_yticks().tolist()\n",
    "    axes[x,y].set_yticks(yticks)\n",
    "    axes[x,y].set_yticklabels([int(obs) for obs in yticks], fontsize=12)\n",
    "    \n",
    "    axes[x,y].legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaa059",
   "metadata": {},
   "source": [
    "Although the probabilities are a bit higher than when I assumed normality, the main message remains the same and the post-publication value premium is either a statistical outlier or the process exhibits a structural mean-shift. However, as I have shown, the hypothesis that the post-publication mean return is zero cannot be rejected (which can also be seen if we shift the boostrapped distributions to the left so that it has a zero mean). Therefore, looking at the mere data, there is no indication of a dead value premium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e3cfa",
   "metadata": {},
   "source": [
    "## 3.&nbsp;Relative&nbsp;Valuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093fa71",
   "metadata": {},
   "source": [
    "While a basket of stocks with the same characteristics (e.g. same magnitude of market beta) should deliver its expected premium over the long term, it can deviate quite a lot from that over the shorter term for various reasons. Valuations of stocks change continuously as the business cycle affects the discount rate, new news getting priced in and many other factors.\n",
    "<br>\n",
    "The shorter the period, the higher is the effect of changing valuations compared to the embedded premium on returns.\n",
    "For example, the daily equity premium is a few basis points and the volatility of around 100 basis points dominates, but over decades, positive and negative shocks cancel each other out and the premium overweighs. Following the financial crisis 2008/09, the realized US equity premium was well above its historical average since it benefited quite a lot from changes in valuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6d3dd",
   "metadata": {},
   "source": [
    "The US equity premium from 1927 to today was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d045594",
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_premium = df[\"Mkt-RF\"].mean()*12\n",
    "print(f\"{equity_premium:.2%},\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c9441",
   "metadata": {},
   "source": [
    "while the average from 2009 to today was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173be125",
   "metadata": {},
   "outputs": [],
   "source": [
    "equity_premium_recent = df.loc[\"2009-01-01\": , \"Mkt-RF\"].mean()*12\n",
    "print(f\"{equity_premium_recent:.2%},\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb89707",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f04c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = ((df[\"Mkt-RF\"].std() * np.sqrt(12)) / np.sqrt(df.loc[\"2009-01-01\": , \"Mkt-RF\"].count()/12))\n",
    "np.round((equity_premium_recent - equity_premium) / sigma, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c815d7eb",
   "metadata": {},
   "source": [
    "standard deviations above its expected value using the full sample mean and assuming normality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93a901",
   "metadata": {},
   "source": [
    "Accordingly, a large portion of US stock returns since 2009 have been the result of changing valuations, measured in terms of the [CAPE](https://www.nber.org/system/files/working_papers/w8221/w8221.pdf) (which is a function of risk aversion, profitability and expected earnings growth), and not by fundamental drivers such as abnormally high realized earnings growth or dividends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac0c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(shiller_data.loc[\"2004-01-01\":])\n",
    "\n",
    "plt.title(\"US CAPE Ratio\", fontsize=18, fontweight=\"bold\")\n",
    "plt.ylabel(\"CAPE Ratio\", fontsize=14)\n",
    "plt.xlabel(\"Year\", fontsize=14)\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf5bc1",
   "metadata": {},
   "source": [
    "In a similar fashion, the realized value premium is the result of three factors:\n",
    "1. The difference of the ex-ante risk premia between cheap and expensive stocks\n",
    "2. The change in relative valuations due to changing expectations\n",
    "3. Unexpected fundamental changes that do not affect the relative valuations, e.g. a one-time shock that affect earnings but do not affect expectations going forward and hence also not affect relative valuations\n",
    "<br>\n",
    "\n",
    "Again, over a long enough sample, (2) and (3) should have little effect, but over the short term, they can be the main driver of the value premium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f60dd77",
   "metadata": {},
   "source": [
    "Starting with the question whether cheap stocks are historically cheap or expensive stocks are historically expensive, we can compute the z-score of the respective valuation metric (book-to-market, earnings/price, cashflow/price). Because I use variable/market value instead of the often used reciprocal (e.g. price/earnings multiple), positive z-scores indicate historically cheap valuations since we get more fundamental value per amount of market value.\n",
    "<br>\n",
    "I use 30th-percentile and 70th-percentile data for expensive and cheap stocks, respectively, provided by Kenneth French on an annual basis computed at the end of June and fill the data in-between with the realized value premium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37530cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spreads = pd.DataFrame(index = df.index)\n",
    "spreads[\"High B/M Ratio\"] = bm_sorted_data[\"Value Weight Average of BE / ME\"][\"Hi 30\"]\n",
    "spreads[\"Low B/M Ratio\"] = bm_sorted_data[\"Value Weight Average of BE / ME\"][\"Lo 30\"]\n",
    "\n",
    "spreads[\"Large High B/M Ratio\"] = bm_size_sorted_data[\"Value Weight Average of BE_FYt-1/ME_June\"][\"BIG HiBM\"][bm_size_sorted_data[\"Value Weight Average of BE_FYt-1/ME_June\"].index.month == 7]\n",
    "spreads[\"Large Low B/M Ratio\"] = bm_size_sorted_data[\"Value Weight Average of BE_FYt-1/ME_June\"][\"BIG LoBM\"][bm_size_sorted_data[\"Value Weight Average of BE_FYt-1/ME_June\"].index.month == 7]\n",
    "spreads[\"Small High B/M Ratio\"] = bm_size_sorted_data[\"Value Weight Average of BE_FYt-1/ME_June\"][\"SMALL HiBM\"][bm_size_sorted_data[\"Value Weight Average of BE_FYt-1/ME_June\"].index.month == 7]\n",
    "spreads[\"Small Low B/M Ratio\"] = bm_size_sorted_data[\"Value Weight Average of BE_FYt-1/ME_June\"][\"SMALL LoBM\"][bm_size_sorted_data[\"Value Weight Average of BE_FYt-1/ME_June\"].index.month == 7]\n",
    "\n",
    "for i, index in enumerate(df.index):\n",
    "    if index in bm_sorted_data[\"Value Weight Returns Monthly\"].index:\n",
    "        if np.isnan(spreads.loc[index, \"High B/M Ratio\"]):\n",
    "            spreads.loc[index, \"High B/M Ratio\"] = spreads.loc[df.index[i-1], \"High B/M Ratio\"] / (1+bm_sorted_data[\"Value Weight Returns Monthly\"].loc[index, \"Hi 30\"]/100)\n",
    "        if np.isnan(spreads.loc[index, \"Low B/M Ratio\"]):\n",
    "            spreads.loc[index, \"Low B/M Ratio\"] = spreads.loc[df.index[i-1], \"Low B/M Ratio\"] / (1+bm_sorted_data[\"Value Weight Returns Monthly\"].loc[index, \"Lo 30\"]/100)\n",
    "    \n",
    "    if index in bm_size_sorted_data[\"Average Value Weighted Returns Monthly\"].index:\n",
    "        if np.isnan(spreads.loc[index, \"Large High B/M Ratio\"]):\n",
    "            spreads.loc[index, \"Large High B/M Ratio\"] = spreads.loc[df.index[i-1], \"Large High B/M Ratio\"] / (1+bm_size_sorted_data[\"Average Value Weighted Returns Monthly\"].loc[index, \"BIG HiBM\"]/100)\n",
    "        if np.isnan(spreads.loc[index, \"Large Low B/M Ratio\"]):\n",
    "            spreads.loc[index, \"Large Low B/M Ratio\"] = spreads.loc[df.index[i-1], \"Large Low B/M Ratio\"] / (1+bm_size_sorted_data[\"Average Value Weighted Returns Monthly\"].loc[index, \"BIG LoBM\"]/100)\n",
    "        if np.isnan(spreads.loc[index, \"Small High B/M Ratio\"]):\n",
    "            spreads.loc[index, \"Small High B/M Ratio\"] = spreads.loc[df.index[i-1], \"Small High B/M Ratio\"] / (1+bm_size_sorted_data[\"Average Value Weighted Returns Monthly\"].loc[index, \"SMALL HiBM\"]/100)\n",
    "        if np.isnan(spreads.loc[index, \"Small Low B/M Ratio\"]):\n",
    "            spreads.loc[index, \"Small Low B/M Ratio\"] = spreads.loc[df.index[i-1], \"Small Low B/M Ratio\"] / (1+bm_size_sorted_data[\"Average Value Weighted Returns Monthly\"].loc[index, \"SMALL LoBM\"]/100)\n",
    "\n",
    "spreads[\"High E/P Ratio\"] = ep_sorted_data[\"Value Weight Average of E / ME\"][\"Hi 30\"]\n",
    "spreads[\"Low E/P Ratio\"] = ep_sorted_data[\"Value Weight Average of E / ME\"][\"Lo 30\"]\n",
    "\n",
    "spreads[\"Large High E/P Ratio\"] = ep_size_sorted_data[\"Value Weight Average of E/P when portfolio is formed\"][\"BIG HiEP\"]\n",
    "spreads[\"Large Low E/P Ratio\"] = ep_size_sorted_data[\"Value Weight Average of E/P when portfolio is formed\"][\"BIG LoEP\"]\n",
    "spreads[\"Small High E/P Ratio\"] = ep_size_sorted_data[\"Value Weight Average of E/P when portfolio is formed\"][\"SMALL HiEP\"]\n",
    "spreads[\"Small Low E/P Ratio\"] = ep_size_sorted_data[\"Value Weight Average of E/P when portfolio is formed\"][\"SMALL LoEP\"]\n",
    "\n",
    "spreads[\"High CF/P Ratio\"] = cfp_sorted_data[\"Value Weight Average of CF / ME\"][\"Hi 30\"]\n",
    "spreads[\"Low CF/P Ratio\"] = cfp_sorted_data[\"Value Weight Average of CF / ME\"][\"Lo 30\"]\n",
    "\n",
    "spreads[\"Large High CF/P Ratio\"] = cfp_size_sorted_data[\"Value Weight Average of CF/P when portfolio is formed\"][\"BIG HiCFP\"]\n",
    "spreads[\"Large Low CF/P Ratio\"] = cfp_size_sorted_data[\"Value Weight Average of CF/P when portfolio is formed\"][\"BIG LoCFP\"]\n",
    "spreads[\"Small High CF/P Ratio\"] = cfp_size_sorted_data[\"Value Weight Average of CF/P when portfolio is formed\"][\"SMALL HiCFP\"]\n",
    "spreads[\"Small Low CF/P Ratio\"] = cfp_size_sorted_data[\"Value Weight Average of CF/P when portfolio is formed\"][\"SMALL LoCFP\"]\n",
    "\n",
    "for i, index in enumerate(df.index):\n",
    "    if index in ep_sorted_data[\"Value Weight Returns Monthly\"].index:\n",
    "        if np.isnan(spreads.loc[index, \"High E/P Ratio\"]):\n",
    "            spreads.loc[index, \"High E/P Ratio\"] = spreads.loc[df.index[i-1], \"High E/P Ratio\"] / (1+ep_sorted_data[\"Value Weight Returns Monthly\"].loc[index, \"Hi 30\"]/100)\n",
    "        if np.isnan(spreads.loc[index, \"Low E/P Ratio\"]):\n",
    "            spreads.loc[index, \"Low E/P Ratio\"] = spreads.loc[df.index[i-1], \"Low E/P Ratio\"] / (1+ep_sorted_data[\"Value Weight Returns Monthly\"].loc[index, \"Lo 30\"]/100)\n",
    "    \n",
    "    if index in ep_size_sorted_data[\"Value Weight Average Returns Monthly\"].index:\n",
    "        if np.isnan(spreads.loc[index, \"Large High E/P Ratio\"]):\n",
    "            spreads.loc[index, \"Large High E/P Ratio\"] = spreads.loc[df.index[i-1], \"Large High E/P Ratio\"] / (1+ep_size_sorted_data[\"Value Weight Average Returns Monthly\"].loc[index, \"BIG HiEP\"]/100)\n",
    "        if np.isnan(spreads.loc[index, \"Large Low E/P Ratio\"]):\n",
    "            spreads.loc[index, \"Large Low E/P Ratio\"] = spreads.loc[df.index[i-1], \"Large Low E/P Ratio\"] / (1+ep_size_sorted_data[\"Value Weight Average Returns Monthly\"].loc[index, \"BIG LoEP\"]/100)\n",
    "        if np.isnan(spreads.loc[index, \"Small High E/P Ratio\"]):\n",
    "            spreads.loc[index, \"Small High E/P Ratio\"] = spreads.loc[df.index[i-1], \"Small High E/P Ratio\"] / (1+ep_size_sorted_data[\"Value Weight Average Returns Monthly\"].loc[index, \"SMALL HiEP\"]/100)\n",
    "        if np.isnan(spreads.loc[index, \"Small Low E/P Ratio\"]):\n",
    "            spreads.loc[index, \"Small Low E/P Ratio\"] = spreads.loc[df.index[i-1], \"Small Low E/P Ratio\"] / (1+ep_size_sorted_data[\"Value Weight Average Returns Monthly\"].loc[index, \"SMALL LoEP\"]/100)\n",
    "\n",
    "    if index in cfp_sorted_data[\"Value Weight Returns Monthly\"].index:\n",
    "        if np.isnan(spreads.loc[index, \"High CF/P Ratio\"]):\n",
    "            spreads.loc[index, \"High CF/P Ratio\"] = spreads.loc[df.index[i-1], \"High CF/P Ratio\"] / (1+cfp_sorted_data[\"Value Weight Returns Monthly\"].loc[index, \"Hi 30\"]/100)\n",
    "        if np.isnan(spreads.loc[index, \"Large Low CF/P Ratio\"]):\n",
    "            spreads.loc[index, \"Low CF/P Ratio\"] = spreads.loc[df.index[i-1], \"Low CF/P Ratio\"] / (1+cfp_sorted_data[\"Value Weight Returns Monthly\"].loc[index, \"Lo 30\"]/100)\n",
    "            \n",
    "    if index in cfp_size_sorted_data[\"Value Weight Average Returns Monthly\"].index:\n",
    "        if np.isnan(spreads.loc[index, \"Large High CF/P Ratio\"]):\n",
    "            spreads.loc[index, \"Large High CF/P Ratio\"] = spreads.loc[df.index[i-1], \"Large High CF/P Ratio\"] / (1+cfp_size_sorted_data[\"Value Weight Average Returns Monthly\"].loc[index, \"BIG HiCFP\"]/100)\n",
    "        if np.isnan(spreads.loc[index, \"Large Low CF/P Ratio\"]):\n",
    "            spreads.loc[index, \"Large Low CF/P Ratio\"] = spreads.loc[df.index[i-1], \"Large Low CF/P Ratio\"] / (1+cfp_size_sorted_data[\"Value Weight Average Returns Monthly\"].loc[index, \"BIG LoCFP\"]/100)\n",
    "        if np.isnan(spreads.loc[index, \"Small High CF/P Ratio\"]):\n",
    "            spreads.loc[index, \"Small High CF/P Ratio\"] = spreads.loc[df.index[i-1], \"Small High CF/P Ratio\"] / (1+cfp_size_sorted_data[\"Value Weight Average Returns Monthly\"].loc[index, \"SMALL HiCFP\"]/100)\n",
    "        if np.isnan(spreads.loc[index, \"Small Low CF/P Ratio\"]):\n",
    "            spreads.loc[index, \"Small Low CF/P Ratio\"] = spreads.loc[df.index[i-1], \"Small Low CF/P Ratio\"] / (1+cfp_size_sorted_data[\"Value Weight Average Returns Monthly\"].loc[index, \"SMALL LoCFP\"]/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d57a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = {\n",
    "    \"High\": pd.DataFrame(index = [\"B/M\", \"E/P\", \"CF/P\"], columns=[\"All\", \"Large\", \"Small\"]),\n",
    "    \"Low\": pd.DataFrame(index = [\"B/M\", \"E/P\", \"CF/P\"], columns=[\"All\", \"Large\", \"Small\"])\n",
    "}\n",
    "z_scores[\"High\"].index.name = \"Cheap Stocks\"\n",
    "z_scores[\"Low\"].index.name = \"Expensive Stocks\"\n",
    "for breakpoint in z_scores.keys():\n",
    "    for var in z_scores[breakpoint].index:\n",
    "        for cap in z_scores[breakpoint].columns:\n",
    "            if cap == \"All\":\n",
    "                name = f\"{breakpoint} {var} Ratio\"\n",
    "            else:\n",
    "                name = f\"{cap} {breakpoint} {var} Ratio\"\n",
    "            z_scores[breakpoint].loc[var, cap] = (spreads[name][-1] - spreads[name].mean()) / spreads[name].std()\n",
    "display(z_scores[\"High\"].applymap(lambda x: round(x, 2)))\n",
    "display(z_scores[\"Low\"].applymap(lambda x: round(x, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8805ea6e",
   "metadata": {},
   "source": [
    "Both cheap and expensive stocks are pricey compared to their historical averages, which, given the current low rate environment, is not suprising. However, expensive stocks seem to be disproportionately expensive based on book-value/market-value and earnings/price ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542dacc4",
   "metadata": {},
   "source": [
    "To gain further insight, we can take a look at the relative valuations of stocks with high book-to-market ratios and those with low book-to-market ratios, both in small- and large-caps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413457cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spreads[\"Valuation Spread All\"] = spreads[\"High B/M Ratio\"] / spreads[\"Low B/M Ratio\"]\n",
    "spreads[\"Valuation Spread Large\"] = spreads[\"Large High B/M Ratio\"] / spreads[\"Large Low B/M Ratio\"]\n",
    "spreads[\"Valuation Spread Small\"] = spreads[\"Small High B/M Ratio\"] / spreads[\"Small Low B/M Ratio\"]\n",
    "\n",
    "figure, axes = plt.subplots(3, figsize=(15,12))\n",
    "plt.subplots_adjust(hspace=0.6)\n",
    "figure.suptitle(\"Relative B/M Valuations of Cheap/Expensive Stocks\", fontsize=18, fontweight=\"bold\", y=0.95)\n",
    "\n",
    "axes[0].plot(spreads[\"Valuation Spread All\"][\"1950-01-01\":],color=\"#4459c2\")\n",
    "axes[0].axvline(x=(2007-1970)*365, label=\"HML Drawdown Start\", color=\"#d66b6b\")\n",
    "\n",
    "axes[0].set_title(\"Large + Small Caps\", size=16)\n",
    "axes[0].set_xlabel(\"Year\", size=14)\n",
    "axes[0].set_ylabel(\"Relative B/M\", size=14)\n",
    "axes[0].tick_params(axis=\"x\", labelsize=13)\n",
    "\n",
    "axes[0].legend(fontsize=11, loc=\"upper right\");\n",
    "\n",
    "axes[1].plot(spreads[\"Valuation Spread Large\"][\"1950-01-01\":], color=\"#4459c2\")\n",
    "axes[1].axvline(x=(2007-1970)*365, label=\"HML Drawdown Start\", color=\"#d66b6b\")\n",
    "\n",
    "axes[1].set_title(\"Large Caps\", size=16)\n",
    "axes[1].set_xlabel(\"Year\", size=14)\n",
    "axes[1].set_ylabel(\"Relative B/M\", size=14)\n",
    "axes[1].tick_params(axis=\"x\", labelsize=13)\n",
    "\n",
    "axes[1].legend(fontsize=11, loc=\"upper right\");\n",
    "\n",
    "axes[2].plot(spreads[\"Valuation Spread Small\"][\"1950-01-01\":], color=\"#4459c2\")\n",
    "axes[2].axvline(x=(2007-1970)*365, label=\"HML Drawdown Start\", color=\"#d66b6b\")\n",
    "\n",
    "axes[2].set_title(\"Small Caps\", size=16)\n",
    "axes[2].set_xlabel(\"Year\", size=14)\n",
    "axes[2].set_ylabel(\"Relative B/M\", size=14)\n",
    "axes[2].tick_params(axis=\"x\", labelsize=13)\n",
    "\n",
    "axes[2].legend(fontsize=11, loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b1867",
   "metadata": {},
   "source": [
    "It becomes clear that relative valuations increased dramatically since the start of the HML drawdown in January 2007 with relative valuations widening between 200% and 250% and that the current level is similar to that of the 2000 dotcom bubble and the great depression in the 1930s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7459474f",
   "metadata": {},
   "source": [
    "The picture looks similar when using net-earnings or cashflows as the fundamental variable instead of book-values, although the widening is not as significant as for book-values. *(Again, the data starts later)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b489b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spreads[\"E/P Valuation Spread All\"] = spreads[\"High E/P Ratio\"] / spreads[\"Low E/P Ratio\"]\n",
    "spreads[\"E/P Valuation Spread Large\"] = spreads[\"Large High E/P Ratio\"] / spreads[\"Large Low E/P Ratio\"]\n",
    "spreads[\"E/P Valuation Spread Small\"] = spreads[\"Small High E/P Ratio\"] / spreads[\"Small Low E/P Ratio\"]\n",
    "spreads[\"CF/P Valuation Spread All\"] = spreads[\"High CF/P Ratio\"] / spreads[\"Low CF/P Ratio\"]\n",
    "spreads[\"CF/P Valuation Spread Large\"] = spreads[\"Large High CF/P Ratio\"] / spreads[\"Large Low CF/P Ratio\"]\n",
    "spreads[\"CF/P Valuation Spread Small\"] = spreads[\"Small High CF/P Ratio\"] / spreads[\"Small Low CF/P Ratio\"]\n",
    "\n",
    "df[\"High E/P Ratio\"] = ep_sorted_data[\"Value Weight Average of E / ME\"][\"Hi 30\"]\n",
    "df[\"Low E/P Ratio\"] = ep_sorted_data[\"Value Weight Average of E / ME\"][\"Lo 30\"]\n",
    "df[\"E/P Valuation Spread All\"] = (df[\"High E/P Ratio\"] / df[\"Low E/P Ratio\"]).ffill()\n",
    "\n",
    "df[\"Large High E/P Ratio\"] = ep_size_sorted_data[\"Value Weight Average of E/P when portfolio is formed\"][\"BIG HiEP\"]\n",
    "df[\"Large Low E/P Ratio\"] = ep_size_sorted_data[\"Value Weight Average of E/P when portfolio is formed\"][\"BIG LoEP\"]\n",
    "df[\"E/P Valuation Spread Large\"] = (df[\"Large High E/P Ratio\"] / df[\"Large Low E/P Ratio\"]).ffill()\n",
    "\n",
    "df[\"Small High E/P Ratio\"] = ep_size_sorted_data[\"Value Weight Average of E/P when portfolio is formed\"][\"SMALL HiEP\"]\n",
    "df[\"Small Low E/P Ratio\"] = ep_size_sorted_data[\"Value Weight Average of E/P when portfolio is formed\"][\"SMALL LoEP\"]\n",
    "df[\"E/P Valuation Spread Small\"] = (df[\"Small High E/P Ratio\"] / df[\"Small Low E/P Ratio\"]).ffill()\n",
    "\n",
    "df[\"High CF/P Ratio\"] = cfp_sorted_data[\"Value Weight Average of CF / ME\"][\"Hi 30\"]\n",
    "df[\"Low CF/P Ratio\"] = cfp_sorted_data[\"Value Weight Average of CF / ME\"][\"Lo 30\"]\n",
    "df[\"CF/P Valuation Spread All\"] = (df[\"High CF/P Ratio\"] / df[\"Low CF/P Ratio\"]).ffill()\n",
    "\n",
    "df[\"Large High CF/P Ratio\"] = cfp_size_sorted_data[\"Value Weight Average of CF/P when portfolio is formed\"][\"BIG HiCFP\"]\n",
    "df[\"Large Low CF/P Ratio\"] = cfp_size_sorted_data[\"Value Weight Average of CF/P when portfolio is formed\"][\"BIG LoCFP\"]\n",
    "df[\"CF/P Valuation Spread Large\"] = (df[\"Large High CF/P Ratio\"] / df[\"Large Low CF/P Ratio\"]).ffill()\n",
    "\n",
    "df[\"Small High CF/P Ratio\"] = cfp_size_sorted_data[\"Value Weight Average of CF/P when portfolio is formed\"][\"SMALL HiCFP\"]\n",
    "df[\"Small Low CF/P Ratio\"] = cfp_size_sorted_data[\"Value Weight Average of CF/P when portfolio is formed\"][\"SMALL LoCFP\"]\n",
    "df[\"CF/P Valuation Spread Small\"] = (df[\"Small High CF/P Ratio\"] / df[\"Small Low CF/P Ratio\"]).ffill()\n",
    "\n",
    "\n",
    "figure, axes = plt.subplots(nrows=3, ncols=2, figsize=(15,9))\n",
    "plt.subplots_adjust(hspace=0.6)\n",
    "figure.suptitle(\"Relative Valuations for E/P and CF/P\", fontsize=18, fontweight=\"bold\", y=0.95)\n",
    "\n",
    "for row, dataset in enumerate((\n",
    "    \"E/P Valuation Spread All\",\n",
    "    \"E/P Valuation Spread Large\",\n",
    "    \"E/P Valuation Spread Small\"\n",
    ")):\n",
    "\n",
    "    axes[row, 0].plot(spreads[dataset], color=\"#4459c2\")\n",
    "    axes[row, 0].axvline(x=(2007-1970)*365, color=\"#d66b6b\")\n",
    "    axes[row, 0].set_title(f\"{dataset} Stocks\", size=16)\n",
    "    axes[row, 0].set_xlabel(\"Year\", size=14)\n",
    "    yticks = [1,2,3,4,5,6,7,8]\n",
    "    axes[row, 0].set_yticks(yticks)\n",
    "    axes[row, 0].set_yticklabels([int(tick) for tick in yticks], size=13)\n",
    "    axes[row, 0].set_ylabel(f\"Relative {dataset.split()[0]}\", size=14)\n",
    "    axes[row, 0].tick_params(axis=\"both\", labelsize=13)\n",
    "\n",
    "for row, dataset in enumerate((\n",
    "    \"CF/P Valuation Spread All\",\n",
    "    \"CF/P Valuation Spread Large\",\n",
    "    \"CF/P Valuation Spread Small\"\n",
    ")):\n",
    "    axes[row, 1].plot(spreads[dataset], color=\"#4459c2\")\n",
    "    axes[row, 1].axvline(x=(2007-1970)*365, color=\"#d66b6b\")\n",
    "    axes[row, 1].set_title(f\"{dataset} Stocks\", size=16)\n",
    "    axes[row, 1].set_xlabel(\"Year\", size=14)\n",
    "    yticks = [1,2,3,4,5,6,7,8]\n",
    "    axes[row, 1].set_yticks(yticks)\n",
    "    axes[row, 1].set_yticklabels([int(tick) for tick in yticks], size=13)\n",
    "    axes[row, 1].set_ylabel(f\"Relative {dataset.split()[0]}\", size=14)\n",
    "    axes[row, 1].tick_params(axis=\"both\", labelsize=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f42fa08",
   "metadata": {},
   "source": [
    "This indicates that the value premium during the last 15 years was to a large extent driven by changing relative valuations between cheap and expensive stocks. Hence, the \"fundamental\" value premium that accounts for unexpected changes in the explaining factors of relative valuations therefore might not have been as bad as it first seems when looking solely at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8233b8",
   "metadata": {},
   "source": [
    "## 4.&nbsp;Explaining&nbsp;Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14369b88",
   "metadata": {},
   "source": [
    "Because the change in relative valuations of cheap and expensive stocks has a big impact on the realized premium, we should analyze the value premium adjusted for changes in the state vector that contains variables that describe the current market environment and affect relative valuations.\n",
    "<br>\n",
    "The naive way to do so is to adjust the premium by subtracting the change in valuation. For example, when the value premium returns 100% and expensive stocks double their relative valuation compared to cheap stocks, the adjusted premium would be zero. However, this would ignore changes that also justify changes in relative valuations (such as expensive stocks becoming more profitable) and the adjusted premium would be biased.\n",
    "<br>\n",
    "A better way is therefore to find variables that affect relative valuations, adjust the valuation spread to those variables and subtract the adjusted valuation spread from the value premium to compute a \"fundamental\" value premium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d735aa2",
   "metadata": {},
   "source": [
    "Starting with the simple pricing equation, any asset price should equal the sum of all excess cashflows it will generate during its lifetime, discounted to the present.\n",
    "<br>\n",
    "<br>\n",
    "$$ Market\\;Value = \\sum \\limits_{t=1} ^{\\infty} \\frac{Free\\;Cashflow_t}{(1+Discount\\;Rate_t)^{t}} $$\n",
    "<br>\n",
    "Assuming a flat term structure of discount rates, a constant growth rate of free-cashflows and that the discount rate is higher than the growth rate, the resulting geometric series can be simplified to\n",
    "<br>\n",
    "<br>\n",
    "$$ Market\\;Value = \\frac{Free\\;Cashflow}{Discount\\;Rate-Growth\\;Rate} $$\n",
    "<br>\n",
    "The discount rate of any asset consists of its risk premium and the rate one would get on a riskless asset.\n",
    "<br>\n",
    "<br>\n",
    "$$ Market\\;Value = \\frac{Free\\;Cashflow}{Riskfree\\;Rate+Risk\\;Premium-Growth\\;Rate} $$\n",
    "<br>\n",
    "The growth rate of free cashflows is governed by the reinvestment rate scaled by profitability. If all earnings are reinvested with a return on equity (and assuming constant leverage) of 10%, then the growth rate of free-cashflows and earnings has to be 10%. Likewise, if only 50% of cashflows are reinvested, the growth rate would be 5%.\n",
    "<br>\n",
    "<br>\n",
    "$$ Market\\;Value = \\frac{Free\\;Cashflow}{Riskfree\\;Rate+Risk\\;Premium-Reinvestment\\;Rate*Return\\;On\\;Equity} $$\n",
    "<br>\n",
    "Rearranging gives\n",
    "<br>\n",
    "<br>\n",
    "$$ \\frac{Free\\;Cashflow}{Market\\;Value} = Riskfree\\;Rate+Risk\\;Premium-Reinvestment\\;Rate*Return\\;On\\;Equity $$\n",
    "<br>\n",
    "Since $Free\\;Cashflow = (1 + Reinvestment\\;Rate) * Earnings$, we get\n",
    "<br>\n",
    "<br>\n",
    "$$ \\frac{Earnings}{Market\\;Value} = (Riskfree\\;Rate+Risk\\;Premium-Reinvestment\\;Rate*Return\\;On\\;Equity) * \\frac{1}{1-Reinvestment\\;Rate} $$\n",
    "<br>\n",
    "Operating Cashflows equal Earnings after Working Capital changes, which in most cases should not be too large and hence can be ignored so that we can also write\n",
    "<br>\n",
    "<br>\n",
    "$$ \\frac{Operating\\;Cashflow}{Market\\;Value} = (Riskfree\\;Rate+Risk\\;Premium-Reinvestment\\;Rate*Return\\;On\\;Equity) * \\frac{1}{1-Reinvestment\\;Rate} $$\n",
    "<br>\n",
    "Book-Values and Earnings are related via the Return on Equity. Thus, we can also consider Book-to-Market ratios:\n",
    "<br>\n",
    "<br>\n",
    "$$ \\frac{Book\\;Value}{Market\\;Value} = (Riskfree\\;Rate+Risk\\;Premium-Reinvestment\\;Rate*Return\\;On\\;Equity) * \\frac{1}{(1-Reinvestment\\;Rate)*Return\\;On\\;Equity} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f9372",
   "metadata": {},
   "source": [
    "The conclusion is that the amount of earnings, cashflows or book-value one gets for every amount of market value increases with the risk-free rate, increases with the risk premium and decreases with the return on equity.\n",
    "<br>\n",
    "Terefore the valuation spread between cheap and expensive stocks (e.g. book-to-market ratios of cheap stocks divided by book-to-market ratios of expensive stocks)\n",
    "1. **increases** with the risk premium differential between cheap and expensive stocks\n",
    "2. **decreases** with the profitability difference between cheap and expensive stocks\n",
    "*The relationship between valuation ratios and reinvestment rate depends on the values of the discount rate and profitability. Because Kenneth French does not provide data on reinvestment rates anyways, I ignore it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c6c1fd",
   "metadata": {},
   "source": [
    "In addition, expensive stocks tend to grow faster than cheap stocks and hence have a higher portion of their net present value in the distant future than cheap stocks. That is why expensive stocks are more interest-rate sensitive (i.e. have a higher duration) than cheap stocks. For example, at a discount rate of 5%, the net present value of 100 dollar in 10 (30) years would be 61.39 (23.14). An increase of 1% in the discount rate would change net present values to 55.84 and 17.41, resulting in relative changes of -9.05% and -24.76%, respectively.\n",
    "<br>\n",
    "Because risk-free rates are generally not constant across all maturities, cashflows across different maturities are discounted with different rates. The implication is that even though the yield curve shifts upwards, cheap stocks could drop more than expensive stocks if short-term rates rise much faster (or drop much less) than long-term rates.\n",
    "<br>\n",
    "We can hence disentangle the partial effects of the yield curve on valuation spreads by stating that\n",
    "\n",
    "3. the **higher** the level of the yield curve (i.e. the T-bill or short-term T-note rate), the **lower** are valuation spreads of cheap and expensive stocks\n",
    "4. the **steeper** the slope of the yield curve (i.e. the higher the difference between long-term rates and short-term rates), the **lower** are valuation spreads of expensive and cheap stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730f349c",
   "metadata": {},
   "source": [
    "Kenneth French provides only operating profitability data for stocks sorted on book-to-market ratios. I therefore report OLS estimates of the yield curve level and slope for earnings/price and cashflow/price sorted stocks but will ignore them in further analysis.\n",
    "<br>\n",
    "To analyze the impact of each variable, I use the 1-year interest rate as the level of the yield curve and the difference between the 10-year interest rate and the 1-year interest rate as the slope of the yield curve.\n",
    "<br>\n",
    "As operating profitability, I use the operating profit divided by book equity, provided by Kennneth French. Because profitability time-series of stocks with the lowest 30% book-to-market ratios exhibit heavy outliers, I use data of the second quintile (book-to-market ratios between 20th and 40th percentile) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a78e4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spread_labels = (\n",
    "    \"Valuation Spread All\",\n",
    "    \"Valuation Spread Large\",\n",
    "    \"Valuation Spread Small\",\n",
    "    \"E/P Valuation Spread All\",\n",
    "    \"E/P Valuation Spread Large\",\n",
    "    \"E/P Valuation Spread Small\",\n",
    "    \"CF/P Valuation Spread All\",\n",
    "    \"CF/P Valuation Spread Large\",\n",
    "    \"CF/P Valuation Spread Small\"\n",
    ")\n",
    "ols_df = pd.DataFrame(index=bm_size_sorted_data[\"Value Weight Average of OP\"].index)\n",
    "for ts in spread_labels:\n",
    "    ols_df[ts] = spreads[ts]\n",
    "ols_df = ols_df.dropna()\n",
    "ols_df[\"1-Year Interest Rate\"] = rate_1yr\n",
    "ols_df[\"10-Year Interest Rate\"] = rate_10yr\n",
    "ols_df[\"Level\"] = ols_df[\"1-Year Interest Rate\"]\n",
    "ols_df[\"Slope\"] = ols_df[\"10-Year Interest Rate\"] - ols_df[\"1-Year Interest Rate\"]\n",
    "ols_df[\"Large Cheap Profitability\"] = bm_size_sorted_data[\"Value Weight Average of OP\"][\"BIG HiBM\"]\n",
    "ols_df[\"Large Expensive Profitability\"] = bm_size_5_5_sorted_data[\"Value Weight Average of OP\"][\"ME5 BM2\"]\n",
    "ols_df[\"Small Cheap Profitability\"] = bm_size_sorted_data[\"Value Weight Average of OP\"][\"SMALL HiBM\"]\n",
    "ols_df[\"Small Expensive Profitability\"] = bm_size_5_5_sorted_data[\"Value Weight Average of OP\"][\"ME1 BM2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17186b7",
   "metadata": {},
   "source": [
    "Starting with the impact of the yield curve level and slope on each valuation spread, I regress the log-spread on the level and slope because changes in the discount rate should have an exponential effect on the price of an asset and hence also on its valuation relative to any fundamental variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_spreads = {}\n",
    "ols_results = pd.DataFrame(columns = [\"Level\", \"Slope\", \"R^2\"], index = spread_labels)\n",
    "ols_results.index.name = \"Significance Level: 10%: *, 5%: **, 1%: ***\"\n",
    "for series in spread_labels:\n",
    "    res = OLS(\n",
    "        np.log(ols_df[series]), \n",
    "        add_constant(ols_df[[\"Level\", \"Slope\"]])\n",
    "    ).fit()\n",
    "    params = res.params\n",
    "    pvalues = res.pvalues\n",
    "    for var in (\"Level\", \"Slope\"):\n",
    "        if pvalues[var] < 0.01:\n",
    "            significance = \"***\"\n",
    "        elif pvalues[var] < 0.05:\n",
    "            significance = \"**\"\n",
    "        elif pvalues[var] < 0.1:\n",
    "            significance = \"*\"\n",
    "        else:\n",
    "            significance = \"\"\n",
    "        ols_results.loc[series, var] = \"{:.2f}{}\".format(params[var], significance)\n",
    "    ols_results.loc[series, \"R^2\"] = f\"{res.rsquared_adj:.2f}\"\n",
    "    adjusted_spreads[series] = res.resid\n",
    "\n",
    "for sample in (\"All\", \"Large\", \"Small\"):\n",
    "    adjusted_spreads[f\"Average Valuation Spread {sample}\"] = (adjusted_spreads[f\"Valuation Spread {sample}\"]*2 + adjusted_spreads[f\"E/P Valuation Spread {sample}\"] + adjusted_spreads[f\"CF/P Valuation Spread {sample}\"])/4\n",
    "ols_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420cba0",
   "metadata": {},
   "source": [
    "Two oberservations are worth noting.\n",
    "1. As expected, both the level and the slope of the yield curve have a signifant effect on valuation spreads for almost all valuation spreads.\n",
    "2. Less variance is explained for earnings/price and cashflow/price computed valuation spreads than for book-to-market valuation spreads. This should be expected since earnings and cashflows are noisy, book values are more persistent and hence expected earnings and cashflows can deviate much more from their respective current value than book-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f1953",
   "metadata": {},
   "source": [
    "For now, I skip the analysis of profitability on each valuation spread as it gives ambiguous results. The yield curve however already has a big impact on valuation spreads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75541901",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(nrows=4, ncols=3, figsize=(16,12))\n",
    "plt.subplots_adjust(hspace=0.6, wspace=0.3)\n",
    "figure.suptitle(\"Residual Relative Valuations after accouting for Interest-rate differences\", fontsize=18, fontweight=\"bold\", y=0.95)\n",
    "\n",
    "for row, variable in enumerate((\"\", \"E/P \", \"CF/P \")):\n",
    "    for col, sample in enumerate((\"All\", \"Large\", \"Small\")):\n",
    "    \n",
    "        axes[row, col].plot(adjusted_spreads[f\"{variable}Valuation Spread {sample}\"], color=\"#4459c2\")\n",
    "        if variable == \"\":\n",
    "            var_name = \"B/M\"\n",
    "        else:\n",
    "            var_name = variable            \n",
    "        axes[row, col].set_title(f\"{var_name} {sample} Stocks\", size=16)\n",
    "        axes[row, col].set_xlabel(\"Year\", size=14)\n",
    "        yticks = [-0.8, -0.4, 0, 0.4, 0.8]\n",
    "        axes[row, col].set_yticks(yticks)\n",
    "        axes[row, col].set_yticklabels([f\"{tick:.0%}\" for tick in yticks], size=13)\n",
    "        axes[row, col].set_ylabel(\"Log Residuals\", size=14)\n",
    "        axes[row, col].tick_params(axis=\"both\", labelsize=13)\n",
    "\n",
    "for col, sample in enumerate((\"All\", \"Large\", \"Small\")):\n",
    "        axes[3, col].plot(adjusted_spreads[f\"Average Valuation Spread {sample}\"], color=\"#4459c2\")      \n",
    "        axes[3, col].set_title(f\"Average {sample} Stocks\", size=16)\n",
    "        axes[3, col].set_xlabel(\"Year\", size=14)\n",
    "        yticks = [-0.8, -0.4, 0, 0.4, 0.8]\n",
    "        axes[3, col].set_yticks(yticks)\n",
    "        axes[3, col].set_yticklabels([f\"{tick:.0%}\" for tick in yticks], size=13)\n",
    "        axes[3, col].set_ylabel(\"Log Residuals\", size=14)\n",
    "        axes[3, col].tick_params(axis=\"both\", labelsize=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd86076",
   "metadata": {},
   "source": [
    "## 5.&nbsp;Time-Series&nbsp;Predictability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ac452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1788d31c",
   "metadata": {},
   "source": [
    "## 6.&nbsp;ETF/Stock&nbsp;Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1016f",
   "metadata": {},
   "source": [
    "To set up the trade, it is crucial to know what type of stocks or ETFs one should look for, i.e. how high or low the multiples should be. I now use the more common reciprocal multiple and report quantile multiples of price/book, price/earnings and price/cashflow multiples *(hence the lower, the better)*, both for the full sample and distinguished between large and small caps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a5e05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multiples_single = pd.DataFrame(columns=[\"P/E\", \"P/CF\", \"P/B\"])\n",
    "for i in range(10, 0, -1):\n",
    "    if i == 1:\n",
    "        multiples_single.loc[\"Highest 10%\", \"P/E\"] = 1/ep_sorted_data[\"Value Weight Average of E / ME\"][\"Lo 10\"].iloc[-1]\n",
    "        multiples_single.loc[\"Highest 10%\", \"P/CF\"] = 1/cfp_sorted_data[\"Value Weight Average of CF / ME\"][\"Lo 10\"].iloc[-1]\n",
    "        multiples_single.loc[\"Highest 10%\", \"P/B\"] = 1/bm_sorted_data[\"Value Weight Average of BE / ME\"][\"Lo 10\"].iloc[-1]\n",
    "    elif i == 10:\n",
    "        multiples_single.loc[\"Lowest 10%\", \"P/E\"] = 1/ep_sorted_data[\"Value Weight Average of E / ME\"][\"Hi 10\"].iloc[-1]\n",
    "        multiples_single.loc[\"Lowest 10%\", \"P/CF\"] = 1/cfp_sorted_data[\"Value Weight Average of CF / ME\"][\"Hi 10\"].iloc[-1]\n",
    "        multiples_single.loc[\"Lowest 10%\", \"P/B\"] = 1/bm_sorted_data[\"Value Weight Average of BE / ME\"][\"Hi 10\"].iloc[-1]\n",
    "    else:\n",
    "        multiples_single.loc[f\"{(i)/10:.0%}\", \"P/E\"] = 1/ep_sorted_data[\"Value Weight Average of E / ME\"][f\"Dec {i}\"].iloc[-1]\n",
    "        multiples_single.loc[f\"{(i)/10:.0%}\", \"P/CF\"] = 1/cfp_sorted_data[\"Value Weight Average of CF / ME\"][f\"Dec {i}\"].iloc[-1]\n",
    "        multiples_single.loc[f\"{(i)/10:.0%}\", \"P/B\"] = 1/bm_sorted_data[\"Value Weight Average of BE / ME\"][f\"Dec {i}\"].iloc[-1]\n",
    "multiples_single = multiples_single.applymap(lambda x: np.round(x, 2))\n",
    "\n",
    "\n",
    "multiples_double = pd.DataFrame(\n",
    "    columns=[\"P/E\", \"P/CF\", \"P/B\"],\n",
    "    index=pd.MultiIndex.from_product([[\"Large\", \"Small\"], [\"Lowest 30%\", \"70%\", \"Highest 30%\"]])\n",
    ")\n",
    "i = 0\n",
    "for size in range(1, -1, -1):\n",
    "    for value in range(2, -1, -1):\n",
    "        multiples_double.loc[multiples_double.index[i], \"P/E\"] = (1/ep_size_sorted_data[\"Value Weight Average of E/P when portfolio is formed\"].iloc[-1, size*3+value])\n",
    "        multiples_double.loc[multiples_double.index[i], \"P/CF\"] = (1/cfp_size_sorted_data[\"Value Weight Average of CF/P when portfolio is formed\"].iloc[-1, size*3+value])\n",
    "        multiples_double.loc[multiples_double.index[i], \"P/B\"] = (1/bm_size_sorted_data[\"Value Weight Average of BE/ME\"].iloc[-1, size*3+value])\n",
    "        i += 1\n",
    "multiples_double = multiples_double.applymap(lambda x: np.round(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d6c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(multiples_single)\n",
    "display(multiples_double)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb505a6",
   "metadata": {},
   "source": [
    "It becomes clear that one should choose a basket of stocks or ETFs with\n",
    "<br>\n",
    "1. price/earnings < 11\n",
    "2. price/cashflows < 10\n",
    "3. price/book < 1.5\n",
    "\n",
    "and avoid or short stocks / ETFs with\n",
    "1. price/earnings > 35\n",
    "2. price/cashflows > 30\n",
    "3. price/book > 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7cf92d",
   "metadata": {},
   "source": [
    "Turning to ETFs for european investors, there are some UCITS-compliant ETFs with significantly positive and negative value-factor loadings, covering the United States, Europe, Emerging Markets and global stock universes.\n",
    "<br>\n",
    "For comparison purposes, I report year-to-date returns in US-Dollar. If someone is long an ETF with US stocks and the EUR/USD exchange rate falls, the ETF would outperform a european counterpart with exactly the same metrics. Therefore, to account for an implicit currency bet, I translate all returns that are not in US-Dollar to USD returns by multiplying the ETF return with the change in the currency rate of that currency to the US-Dollar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_factors = FrenchReader(\"Developed_5_Factors\").read()[\"Main\"] / 100\n",
    "eu_factors = FrenchReader(\"Europe_5_Factors\").read()[\"Main\"] / 100\n",
    "em_factors = FrenchReader(\"Emerging_5_Factors\").read()[\"Main\"] / 100\n",
    "\n",
    "eur_usd = YahooReader(\"EURUSD=X\").historical_data(frequency=\"1mo\")[\"data\"][\"simple_returns\"]\n",
    "eur_usd.index = [pd.to_datetime(item.date()) for item in eur_usd.index]\n",
    "\n",
    "gbp_usd = YahooReader(\"GBPUSD=X\").historical_data(frequency=\"1mo\")[\"data\"][\"simple_returns\"]\n",
    "gbp_usd.index = [pd.to_datetime(item.date()) for item in gbp_usd.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065911dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long ETFs\n",
    "long_etfs = {\n",
    "    \"IE00BSPLC413\": \"ZPRV.DE\",\n",
    "    \"IE00BJRCLL96\": \"JPGL.DE\",\n",
    "    \"IE00BP3QZB59\": \"IS3S.DE\",\n",
    "    \"IE00BJRCLK89\": \"JPUS\",\n",
    "    \"IE00BF2B0K52\": \"FLXE.DE\",\n",
    "    \"IE00BG0SKF03\": \"5MVL.DE\",\n",
    "    \"IE00BSPLC298\": \"ZPRX.DE\"\n",
    "}\n",
    "\n",
    "long_etf_exposures = pd.DataFrame(columns=[\"Name\", \"ISIN\", \"MKT\", \"SMB\", \"HML\", \"RMW\", \"R2\", \"YTD Dollar Return\"])\n",
    "long_etf_exposures.index.name = \"Ticker\"\n",
    "etf_list = []\n",
    "for isin, ticker in long_etfs.items():\n",
    "    reader = YahooReader(ticker)\n",
    "    name = reader.name\n",
    "    long_etf_exposures.loc[ticker, [\"Name\", \"ISIN\"]] = [name, isin]\n",
    "    \n",
    "    historical_data = reader.historical_data(frequency=\"1mo\")\n",
    "    series = historical_data[\"data\"][\"simple_returns\"]\n",
    "    series.index = [pd.to_datetime(item.date()) for item in series.index]\n",
    "    \n",
    "    currency = historical_data[\"information\"][\"currency\"]    \n",
    "    if currency == \"EUR\":\n",
    "        series = ((1+series) * (1+eur_usd) - 1).dropna()\n",
    "    \n",
    "    ytd_return = (1+series[\"2022-01-01\":]).cumprod()[-1] - 1\n",
    "    ytd_return = f\"{ytd_return:.2%}\"\n",
    "    \n",
    "    series.name = ticker\n",
    "    etf_list.append(series)\n",
    "    \n",
    "    if ticker in (\"ZPRV.DE\", \"JPUS\"):\n",
    "        temp_df = pd.concat([series, ff5], axis=1).dropna()\n",
    "    elif ticker in (\"JPGL.DE\", \"IS3S.DE\"):\n",
    "        temp_df = pd.concat([series, global_factors], axis=1).dropna()\n",
    "    elif ticker == \"ZPRX.DE\":\n",
    "        temp_df = pd.concat([series, eu_factors], axis=1).dropna()\n",
    "    elif ticker in (\"FLXE.DE\", \"5MVL.DE\"):\n",
    "        temp_df = pd.concat([series, em_factors], axis=1).dropna()\n",
    "    \n",
    "    ols = OLS(temp_df[ticker], add_constant(temp_df[[\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\"]])).fit()\n",
    "    long_etf_exposures.loc[ticker, [\"MKT\", \"SMB\", \"HML\", \"RMW\", \"R2\", \"YTD Dollar Return\"]] = list(ols.params[1:].apply(lambda x: round(x, 2))) + [round(ols.rsquared, 2), ytd_return]\n",
    "\n",
    "# Short ETFs\n",
    "short_etfs = {\n",
    "    \"DE000ETFL037\": \"EL4C.DE\",\n",
    "    \"IE00BDVPNG13\": \"WTAI.L\",\n",
    "    \"IE00BLPK3577\": \"WCBR.MI\",\n",
    "    \"IE00BJGWQN72\": \"KLWD.L\"\n",
    "}\n",
    "\n",
    "short_etf_exposures = pd.DataFrame(columns=[\"Name\", \"ISIN\", \"MKT\", \"SMB\", \"HML\", \"RMW\", \"R2\", \"YTD Dollar Return\"])\n",
    "short_etf_exposures.index.name = \"Ticker\"\n",
    "etf_list = []\n",
    "for isin, ticker in short_etfs.items():\n",
    "    reader = YahooReader(ticker)\n",
    "    name = reader.name\n",
    "    short_etf_exposures.loc[ticker, [\"Name\", \"ISIN\"]] = [name, isin]\n",
    "    \n",
    "    historical_data = reader.historical_data(frequency=\"1mo\")\n",
    "    series = historical_data[\"data\"][\"simple_returns\"]\n",
    "    series.index = [pd.to_datetime(item.date()) for item in series.index]\n",
    "    \n",
    "    currency = historical_data[\"information\"][\"currency\"]\n",
    "    if currency == \"EUR\":\n",
    "        series = ((1+series) * (1+eur_usd) - 1).dropna()\n",
    "    elif currency == \"GBp\":\n",
    "        series = ((1+series) * (1+gbp_usd) - 1).dropna()\n",
    "    \n",
    "    ytd_return = (1+series[\"2022-01-01\":]).cumprod()[-1] - 1\n",
    "    ytd_return = f\"{ytd_return:.2%}\"\n",
    "    \n",
    "    series.name = ticker\n",
    "    etf_list.append(series)\n",
    "    \n",
    "    if ticker == \"EL4C.DE\":\n",
    "        temp_df = pd.concat([series, eu_factors], axis=1).dropna()\n",
    "    elif ticker in (\"WTAI.L\", \"WCBR.MI\", \"KLWD.L\"):\n",
    "        temp_df = pd.concat([series, ff5], axis=1).dropna()\n",
    "    \n",
    "    ols = OLS(temp_df[ticker], add_constant(temp_df[[\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\"]])).fit()\n",
    "    short_etf_exposures.loc[ticker, [\"MKT\", \"SMB\", \"HML\", \"RMW\", \"R2\", \"YTD Dollar Return\"]] = list(ols.params[1:].apply(lambda x: round(x, 2))) + [round(ols.rsquared, 2), ytd_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPotential Long ETF Candidates\\n\", \"_\"*40, sep=\"\")\n",
    "display(long_etf_exposures)\n",
    "\n",
    "print(\"\\nPotential Short ETF Candidates\\n\", \"_\"*40, sep=\"\")\n",
    "display(short_etf_exposures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908c6fc",
   "metadata": {},
   "source": [
    "Putting it together, all long candidates have meaningful value-factor loadings, although the short candidates have twice as high loadings in magnitude. Thus there are three possibilities to harvest the value premium going-forward:\n",
    "1. Go long a single or multiple long candidates. Given the negative correlation between the market premium and value premium, the sharpe ratio should be superior than that of a market portfolio. However, the performance boost is limited.\n",
    "<br><br>\n",
    "2. Set up a market-neutral trade that mimicks the value premium by going long a basket of long candidates and short a basket of short candidates. With that, value-factor loadings should be around 1 and the trade also gives high exposure to the profitability factor (RMW). In line with the value premium, this trade would have yielded 15-30% excess return year-to-date, depending on the etf selection.\n",
    "<br><br>\n",
    "3. Pursue a mix of 1. and 2. by leveraging up long candidates to >100% of the portfolio and simultaneously short a basket of short candidates such that the beta of the portfolio remains around 1. The result is a portfolio like that in alternative 1, but with much higher value- and profitability-factor loadings. Hence, both the sharpe ratio and expected return should be higher than in alternative 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6d1efb",
   "metadata": {},
   "source": [
    "## 7.&nbsp;Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397acc7f",
   "metadata": {},
   "source": [
    "In this notebook, I have analyzed the value premium in the United States. I have shown that the claim that it has vanished due to arbitrage is overblown and that a look at the past performance can be misleading. In fact, a simple stochastic analysis revealed that the hypothesis of a zero-mean value premium can be easily rejected.\n",
    "<br><br>\n",
    "Because prices and expected returns are inversely related, all else being equal, and since valuation differentials between cheap and expensive stocks are at historically high levels, one can conclude that either the expected value premium or the difference of earnings growth between expensive and cheap stocks is historically high. The latter has yet to turn into reality and given the 2000 dotcom bubble should be viewed with caution.\n",
    "<br>\n",
    "Even after accounting for today's low interest rates and the diffent durations of cheap and expensive stocks results in historically high valuation spreads and given the fast tightening of the central bank this year, interest-rate adjusted spreads remain that high.\n",
    "<br><br>\n",
    "Finally, an investor aiming at high exposure to the value factor should look for stocks with reasonably low multiples (i.e. P/E < 11, P/CF < 10, P/B < 1.5) and avoid or in a market-neutral trade short stocks with high multiples (P/E > 35, P/CF > 30, P/B > 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af69a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53d8f185c1390ea0a8248ce0a5c0ba235224e0f874044819f6c065bb8a249464"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
